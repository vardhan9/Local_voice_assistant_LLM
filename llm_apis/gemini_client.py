import google.generativeai as genai
import os
import base64
import httpx

class GeminiClient:
    """Client for interacting with Google Gemini API."""
    def __init__(self, verbose=False):
        genai.configure(api_key=os.environ["GEMINI_API_KEY"])
        self.verbose = verbose

    def stream_completion(self, messages, model, **kwargs):
        """Get completion from Google Gemini API.

        Args:
            messages (list): List of messages.
            model (str): Model for completion.
            **kwargs: Additional keyword arguments including:
                - temperature: float (0.0 to 2.0)
                - max_tokens: int

        Yields:
            str: Text generated by the Gemini API.
        """
        try:
            # Extract generation config parameters
            generation_config = {
                "temperature": kwargs.pop('temperature', 0.7),
                "max_output_tokens": kwargs.pop('max_tokens', 2048)
            }

            gemini_model = genai.GenerativeModel(model)

            # Process messages to handle multimodal content
            prompt = []
            for message in messages:
                if isinstance(message.get('content'), str):
                    prompt.append(message['content'])
                elif isinstance(message.get('content'), list):
                    for item in message['content']:
                        if item.get('type') == 'image':
                            prompt.append({
                                "mime_type": item['source']['media_type'],
                                "data": item['source']['data']
                            })
                        else:
                            prompt.append(item.get('text', ''))

            # Generate content with generation config
            response = gemini_model.generate_content(
                prompt,
                generation_config=generation_config,
                stream=True,
                **kwargs
            )

            for chunk in response:
                if chunk.candidates:
                    for candidate in chunk.candidates:
                        if candidate.content and candidate.content.parts:
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    yield part.text

        except Exception as e:
            if self.verbose:
                import traceback
                traceback.print_exc()
            else:
                print(f"An error occurred streaming completion from Gemini: {e}")
            raise RuntimeError(f"An error occurred streaming completion from Gemini: {e}")

# Test the GeminiClient
if __name__ == "__main__":
    client = GeminiClient(verbose=True)

    # Test text only
    messages = [
        {
            "role": "system",
            "content": "Be precise and concise."
        },
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ]
    model = "gemini-1.5-flash"

    print("\nText-only Response:")
    for chunk in client.stream_completion(messages, model):
        print(chunk, end='', flush=True)
    print()

    # Test multimodal
    image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
    image_media_type = "image/jpeg"
    try:
        image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")
    except httpx.RequestError as e:
        print(f"An error occurred while fetching the image: {e}")
        exit()


    messages = [
        {
            "role": "system",
            "content": "Respond only in rhyming couplets."
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Should I eat this?"},
                {
                    "type": "image",
                    "source": {
                        "media_type": image_media_type,
                        "data": image_data
                    }
                }
            ]
        }
    ]

    print("\nMultimodal Response:")
    for chunk in client.stream_completion(messages, model):
        print(chunk, end='', flush=True)
    print()
